{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1BHRAe1xPq"
      },
      "source": [
        "# Install the New Worker- BETA!!\n",
        "### Remember to first set your API KEY and worker name\n",
        "#### You can serve different models, simply change the name in models_to_load to match the model you want, you can check either https://aqualxx.github.io/stable-ui/workers, in the models tab, or https://tinybots.net/artbot/info/models, just copy paste the name (it's set to Deliberate by default)\n",
        "#### You can also change max_power and see how high you can go, it's set to 20 by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## Only to obtain the models names from the horde. If you want it, tick the checkbox here and run this cell. It does nothing for the worker, it will simply print a list of all the models { display-mode: \"form\" }\n",
        "get_models = False # @param {type:\"boolean\"}\n",
        "\n",
        "if (get_models):\n",
        "\n",
        "  !wget -q -O /content/stable_diffusion.json https://raw.githubusercontent.com/Haidra-Org/AI-Horde-image-model-reference/main/stable_diffusion.json\n",
        "\n",
        "  with open(\"/content/stable_diffusion.json\", 'r') as file:\n",
        "      lines = file.readlines()\n",
        "\n",
        "  delete_name = '        \"name\": '\n",
        "  delete_enter = '\\n'\n",
        "  model_list = []\n",
        "\n",
        "  for i, line in enumerate(lines):\n",
        "      if delete_name in line:\n",
        "          #print(f\"{line.replace(delete_name, '').replace(delete_enter,'')}\")\n",
        "          #model_list.append = [f\"{line.replace(delete_name, '').replace(delete_enter,'')}\"]\n",
        "          model_list.append(line.replace(delete_name, '').replace(delete_enter,''))\n",
        "\n",
        "  model_list.sort()  # Sort the model_list in alphabetical order\n",
        "\n",
        "  #print(model_list)\n",
        "  print(' '.join(model_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title # Chronometer (time listener) { display-mode: \"form\" }\n",
        "%%javascript\n",
        "const listenerChannel = new BroadcastChannel('channel');\n",
        "listenerChannel.onmessage = (msg) => {\n",
        "  const div = document.createElement('div');\n",
        "  div.textContent = msg.data;\n",
        "  document.body.innerHTML = '';\n",
        "  document.body.appendChild(div);\n",
        "};\n",
        "\n",
        "const div = document.createElement('listener');\n",
        "document.body.appendChild(div);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title # Chronometer (time sender) { display-mode: \"form\" }\n",
        "%%javascript\n",
        "const senderChannel = new BroadcastChannel('channel');\n",
        "\n",
        "let startTime = new Date().getTime(); // Variable to store the start time\n",
        "\n",
        "function formatTime(seconds) {\n",
        "  const hours = Math.floor(seconds / 3600);\n",
        "  const minutes = Math.floor((seconds % 3600) / 60);\n",
        "  const remainingSeconds = seconds % 60;\n",
        "  return `${hours} hours, ${minutes} minutes, ${remainingSeconds} seconds`;\n",
        "}\n",
        "\n",
        "function updateTime() {\n",
        "  const currentTime = new Date().getTime();\n",
        "  const elapsedSeconds = Math.floor((currentTime - startTime) / 1000);\n",
        "  const formattedTime = formatTime(elapsedSeconds);\n",
        "  senderChannel.postMessage(`Time elapsed: ${formattedTime}`);\n",
        "}\n",
        "\n",
        "setInterval(updateTime, 1000)\n",
        "const div = document.createElement('sender');\n",
        "div.textContent = \"Running timer! These 2 cells will run alongside the worker, keeping track of the time elapsed since starting the notebook. Closing this output wil stop the timer (at least, I think)!\";\n",
        "document.body.appendChild(div);\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-20T23:06:35.113455Z",
          "iopub.status.busy": "2023-10-20T23:06:35.112701Z"
        },
        "id": "qHYIcsqJ1xPs",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 0.- This cell will set the variables and rerun the worker if you stopped it, but only if everything else was installed already { display-mode: \"code\" }\n",
        "\n",
        "#### For right now, THESE are the only variables that we care about\n",
        "worker_name = \"\"\n",
        "api_key = \"\"\n",
        "max_power = 20\n",
        "safety_on_gpu = True\n",
        "models_to_load = [\"Deliberate\"]\n",
        "allow_lora = True\n",
        "allow_controlnet = True\n",
        "nsfw = True\n",
        "censor_nsfw = False\n",
        "\n",
        "##############################\n",
        "##### HIGHLY EXPERIMENTAL - OPTIONAL, disabled by default ####\n",
        "# Colab exclusive, for now, inject loras and textual inversions through prompt so they can be used in similar fashion to A1111\n",
        "# for loras: <lora:civitaiID:lora_strength:lora_clip, Anything you want here, to identify this lora>\n",
        "# for textual inversions: (embedding:civitaiID:ti_strength)\n",
        "experimental = False\n",
        "# why? because some front-ends don't support loras/tis, so instead we let the worker handle that part and now EVERYONE can use them,\n",
        "# no  matter what client they use to access the horde\n",
        "#### HIGHLY EXPERIMENTAL - OPTIONAL, disabled by default  ####\n",
        "##############################\n",
        "\n",
        "####For right now, THESE are the only variables that we care about\n",
        "\n",
        "\n",
        "queue_size = 1\n",
        "max_threads = 1\n",
        "horde_url = \"https://aihorde.net\"\n",
        "allow_painting = False\n",
        "dynamic_models = False\n",
        "models_to_skip = [\"stable_diffusion_inpainting\", \"stable_diffusion_2.1\",  \"stable_diffusion_2.0\"]\n",
        "allow_post_processing = False\n",
        "priority_usernames = []\n",
        "blacklist = []\n",
        "censorlist = []\n",
        "allow_img2img = True\n",
        "allow_unsafe_ip = True\n",
        "number_of_dynamic_models = 0\n",
        "max_models_to_download = 10\n",
        "forms = [\"caption\",\"nsfw\",\"interrogation\",\"post-process\"]\n",
        "\n",
        "\n",
        "current_path = \"/content/\"\n",
        "worker_path = current_path + \"horde-worker-reGen/\"\n",
        "bridgeData_file = worker_path + \"bridgeData.yaml\"\n",
        "\n",
        "import os\n",
        "\n",
        "#@markdown To reduce the level of verbosity/show reduced logs (Colab uses more and more of your PC RAM as the logs grow longer, but this will help with that)\n",
        "Short_Logs = True #@param {type:\"boolean\"}\n",
        "logs = Short_Logs\n",
        "\n",
        "if not os.path.exists(bridgeData_file):\n",
        "    print (\"bridgeData.yaml file not found. Proceed to install the worker.\")\n",
        "else:\n",
        "    print (\"bridgeData.yaml file found. Recreating bridgeData.yaml and restarting the worker.\")\n",
        "    create_yaml()\n",
        "    !cd /content\n",
        "    !source ../regen/bin/activate;python download_models.py\n",
        "    !cd /content\n",
        "    \n",
        "    if (Short_Logs):\n",
        "        !source ../regen/bin/activate;python run_worker.py -vv\n",
        "    else:\n",
        "        !source ../regen/bin/activate;python run_worker.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVecpjOM1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 1.- Virtual Environment  { display-mode: \"form\" }\n",
        "\n",
        "!apt-get update\n",
        "!apt install python3.10-venv\n",
        "!python -m venv regen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzwQDVd_1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 2.- Remove the worker if it exists, then, Clone the regen worker { display-mode: \"form\" }\n",
        "\n",
        "!cd /content;rm -r horde-worker-reGen\n",
        "!cd /content;git clone https://github.com/Haidra-Org/horde-worker-reGen.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC4lce-51xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 3.- Install requirements for cuda 11.8 { display-mode: \"form\" }\n",
        "\n",
        "!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.118.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUSwcaXc1xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 4.- Work around so hordelib installs @ cuda 11.8 { display-mode: \"form\" }\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.hordelib.txt\n",
        "\n",
        "!source regen/bin/activate;pip install hordelib --no-deps\n",
        "#!source regen/bin/activate;pip install hordelib~=2.3.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-I1XNxx1xPv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 5.- Create .yaml config file { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "\n",
        "%cd $worker_path\n",
        "print (\"Creating bridgeData.yaml file.\")\n",
        "\n",
        "def create_yaml():\n",
        "\n",
        "    from yaml import load, dump\n",
        "\n",
        "    def make_yaml_sublist(list_to_convert: list[str]):\n",
        "        sublist_yaml = dump(list_to_convert)\n",
        "        sublist_yaml = \"\\n\" + sublist_yaml\n",
        "        return sublist_yaml\n",
        "\n",
        "\n",
        "\n",
        "    data = f\"\"\"horde_url: \"{horde_url}\"\n",
        "api_key: \"{api_key}\"\n",
        "priority_usernames: []\n",
        "max_threads: {max_threads}\n",
        "queue_size: {queue_size}\n",
        "safety_on_gpu: {safety_on_gpu}\n",
        "require_upfront_kudos: false\n",
        "cycle_process_on_model_change: true\n",
        "dreamer_name: \"{worker_name}\"\n",
        "max_power: {max_power}\n",
        "nsfw: {nsfw.__str__().lower()}\n",
        "censor_nsfw: {censor_nsfw}\n",
        "blacklist: {blacklist}\n",
        "censorlist: {censorlist}\n",
        "allow_img2img: {allow_img2img.__str__().lower()}\n",
        "allow_painting: {allow_painting.__str__().lower()}\n",
        "allow_unsafe_ip: true\n",
        "allow_post_processing: {allow_post_processing.__str__().lower()}\n",
        "allow_controlnet: {allow_controlnet.__str__().lower()}\n",
        "allow_lora: {allow_lora.__str__().lower()}\n",
        "max_lora_cache_size: 20\n",
        "dynamic_models: false\n",
        "number_of_dynamic_models: 0\n",
        "max_models_to_download: 10\n",
        "stats_output_frequency: 30\n",
        "cache_home: \"./\"\n",
        "always_download: true\n",
        "temp_dir: \"./tmp\"\n",
        "disable_terminal_ui: True\n",
        "vram_to_leave_free: \"80%\"\n",
        "ram_to_leave_free: \"80%\"\n",
        "disable_disk_cache: false\n",
        "models_to_load: {make_yaml_sublist(models_to_load)}\n",
        "models_to_skip: {make_yaml_sublist(models_to_skip)}\n",
        "suppress_speed_warnings: false\n",
        "forms:\n",
        "- \"caption\"\n",
        "- \"nsfw\"\n",
        "- \"interrogation\"\n",
        "- \"post-process\"\n",
        "\"\"\"\n",
        "\n",
        "    with open(bridgeData_file, \"w\") as text_file:\n",
        "        text_file.write(data)\n",
        "\n",
        "    print (\"bridgeData.yaml file created.\")\n",
        "\n",
        "create_yaml()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## EXPERIMENTAL - Lora/TI prompt injection and Hires-Fix Denoise Strength and initial image size control { display-mode: \"form\" }\n",
        "\n",
        "# To inject loras and tis through the prompt like auto1111 and other UIs\n",
        "\n",
        "# To undo the experimental changes and disable the loras/tis injection, uncomment the next 2 lines (delete the #) and run this cell again, then rerun the worker (the last cell)\n",
        "#!wget -O /content/regen/lib/python3.10/site-packages/hordelib/horde.py https://raw.githubusercontent.com/Haidra-Org/hordelib/v2.2.3/hordelib/horde.py\n",
        "#experimental = False\n",
        "\n",
        "\n",
        "# To inject loras and tis through the prompt, we search in horde.py for \"search_string\" and write \"new_code\" above it, just because\n",
        "\n",
        "horde_py_path = '/content/regen/lib/python3.10/site-packages/hordelib/horde.py'\n",
        "search_string = '        # Negative and positive prompts are merged together'\n",
        "new_code = r'''\n",
        "        #\n",
        "        # To inject Loras and TIs through the prompt\n",
        "        # Import regular expressions\n",
        "        import re\n",
        "\n",
        "        # Get the lora(s) from the prompt\n",
        "        matches = re.findall(r\"<lora:(-?\\d+:-?\\d+\\.\\d+:-?\\d+\\.\\d+).*?>\", payload.get(\"prompt\"))\n",
        "        lora_array = []\n",
        "        # Array containing all loras in the prompt\n",
        "        for match in matches:\n",
        "            lora_array.append(match)\n",
        "        # Remove the lora(s) from the prompt\n",
        "        payload[\"prompt\"] = re.sub(r\"<lora:-?\\d+:-?\\d+\\.\\d+:-?\\d+\\.\\d+.*?>\", \"\", payload[\"prompt\"])\n",
        "        \n",
        "        # Append the new lora(s) to payload[\"loras\"] \n",
        "        for lora_string in lora_array:\n",
        "            attributes = [\"name\", \"model\", \"clip\"]\n",
        "            values = lora_string.split(':')\n",
        "            lora_dict = dict(zip(attributes, values))\n",
        "            payload[\"loras\"].append(lora_dict)\n",
        "\n",
        "        # Get the ti(s) from the prompt\n",
        "        matches = re.findall(r\"embedding:(-?\\d+:-?\\d+\\.\\d+)\", payload.get(\"prompt\"))\n",
        "        ti_array = []\n",
        "        # Array containing all loras in the prompt\n",
        "        for match in matches:\n",
        "            ti_array.append(match)\n",
        "        \n",
        "        # Append the new TI(s) to payload[\"tis\"] \n",
        "        for ti_string in ti_array:\n",
        "            attributes = [\"name\", \"strength\"]\n",
        "            # if inject_ti is necessary, use this next line instead\n",
        "            #attributes = [\"name\", \"inject_ti\", \"strength\"]\n",
        "            values = ti_string.split(':')\n",
        "            ti_dict = dict(zip(attributes, values))\n",
        "            payload[\"tis\"].append(ti_dict)\n",
        "\n",
        "        # To inject Loras and TIs through the prompt\n",
        "        #\n",
        "\n",
        "        #\n",
        "        # To change hires_fix_denoising_strength\n",
        "        import re\n",
        "        # Get the value from the prompt\n",
        "        match = re.search(r\"<hires_fix:(\\d+\\.\\d+)>\", payload.get(\"prompt\"))\n",
        "        if match:\n",
        "            payload[\"hires_fix_denoising_strength\"] = match.group(1)\n",
        "            payload[\"prompt\"] = re.sub(r\"<hires_fix:(\\d+\\.\\d+)>\", \"\", payload[\"prompt\"])\n",
        "\n",
        "        # To change hires_fix_denoising_strength\n",
        "        #\n",
        "'''\n",
        "\n",
        "search_string_dimensions = '                newwidth, newheight = ImageUtils.calculate_source_image_size(width, height)'\n",
        "new_code_dimensions = r'''                #newwidth, newheight = ImageUtils.calculate_source_image_size(width, height)\n",
        "                #\n",
        "                # To change hires-fix initial dimensions\n",
        "                import re\n",
        "                # Get the dimensions from the prompt\n",
        "                match = re.search(r\"<width\\*height:(\\d+\\*\\d+)>\", payload.get(\"prompt\"))\n",
        "                if match:\n",
        "                    partial_match = match.group(1)\n",
        "                    newwidth = partial_match.split('*')[0]\n",
        "                    newheight = partial_match.split('*')[1]\n",
        "                    payload[\"prompt\"] = re.sub(r\"<width\\*height:(\\d+\\*\\d+)>\", \"\", payload[\"prompt\"])\n",
        "                else:\n",
        "                    newwidth, newheight = ImageUtils.calculate_source_image_size(width, height)\n",
        "\n",
        "                # To change hires-fix initial dimensions\n",
        "                #\n",
        "'''\n",
        "\n",
        "# only execute this if experimental is True\n",
        "if (experimental):\n",
        "    # read the file\n",
        "    with open(horde_py_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    found_line_number = -1\n",
        "\n",
        "    # find the target line \"search_string\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.insert(found_line_number - 1, new_code)\n",
        "    \n",
        "    found_line_number = -1\n",
        "\n",
        "    # find the target line \"search_string_dimensions\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_dimensions in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_dimensions\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_dimensions)\n",
        "\n",
        "    # rewrite horde.py\n",
        "    with open(horde_py_path, 'w') as file:\n",
        "        file.writelines(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## EXPERIMENTAL - Lora/TI injection fix, Lora size limit up (220MB -> 1GB) and login-gated Loras support { display-mode: \"form\" }\n",
        "\n",
        "# Modify lora.py as a fix for the worker update of 12/19/2023. Also, increase lora size limit to 1gb and add support for login-gated loras\n",
        "\n",
        "# To undo the experimental changes, uncomment the next 2 lines (delete the #) and run this cell again, then rerun the worker (the last cell)\n",
        "#!wget -O /content/regen/lib/python3.10/site-packages/hordelib/model_manager/lora.py https://raw.githubusercontent.com/Haidra-Org/hordelib/v2.2.3/hordelib/model_manager/lora.py\n",
        "#experimental = False\n",
        "\n",
        "lora_py_path = '/content/regen/lib/python3.10/site-packages/hordelib/model_manager/lora.py'\n",
        "\n",
        "search_string = '                                self.delete_oldest_lora()'\n",
        "new_code = r'''                                # I added this logger to check it and it is indeed what's deleting the lora\n",
        "                                logger.error(\n",
        "                                  f\"This thinks it's full or whatever\",\n",
        "                                )\n",
        "                                # I commented this next line to skip the lora delete and it started wokring as I wanted\n",
        "                                #self.delete_oldest_lora()\n",
        "'''\n",
        "\n",
        "search_string_2 = '        if lora[\"adhoc\"] and lora[\"size_mb\"] > 220:'\n",
        "new_code_2 = f'''        if lora[\"adhoc\"] and lora[\"size_mb\"] > 1024:\\n'''\n",
        "\n",
        "\n",
        "search_string_3 = '                    response = requests.get(lora[\"url\"], timeout=self.REQUEST_DOWNLOAD_TIMEOUT)'\n",
        "new_code_3 = f'''                    response = requests.get(lora[\"url\"] + \"?token=cae554bcc138d97a9323856c2dee1158\", timeout=self.REQUEST_DOWNLOAD_TIMEOUT)\\n'''\n",
        "\n",
        "\n",
        "\n",
        "#search_string_2 = '                if lora[\"versions\"][lora_version][\"adhoc\"] and lora[\"versions\"][lora_version][\"size_mb\"] > 220:'\n",
        "#new_code_2 = f'''        if lora[\"versions\"][lora_version][\"adhoc\"] and lora[\"versions\"][lora_version][\"size_mb\"] > 1024:\\n'''\n",
        "\n",
        "#search_string_3 = '                    response = requests.get(lora[\"versions\"][version][\"url\"], timeout=self.REQUEST_DOWNLOAD_TIMEOUT)'\n",
        "#new_code_3 = f'''                    response = requests.get(lora[\"versions\"][version][\"url\"] + \"?token=cae554bcc138d97a9323856c2dee1158\", timeout=self.REQUEST_DOWNLOAD_TIMEOUT)\\n'''\n",
        "\n",
        "\n",
        "# only execute this if experimental is True\n",
        "if (experimental):\n",
        "    # read the file\n",
        "    with open(lora_py_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    found_line_number = -1\n",
        "\n",
        "    ### fix for lora injection \n",
        "    # find the target line \"search_string\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code)\n",
        "\n",
        "    ### to increase lora size limit\n",
        "    # find the target line \"search_string_2\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_2 in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_2\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_2)\n",
        "\n",
        "    ### to allow login-gated loras\n",
        "    # find the target line \"search_string_3\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_3 in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_3\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_3)\n",
        "\n",
        "\n",
        "    # rewrite lora.py\n",
        "    with open(lora_py_path, 'w') as file:\n",
        "        file.writelines(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## EXPERIMENTAL - Show popped job and extra info { display-mode: \"form\" }\n",
        "\n",
        "# To modify process_manager.py to show popped job and extra info.\n",
        "# Repeat the above to modify process_manager.py as a fix for the update of 12/19/2023\n",
        "\n",
        "# To undo the experimental changes, uncomment the next 2 lines (delete the #) and run this cell again, then rerun the worker (the last cell)\n",
        "#!wget -O /content/horde-worker-reGen/horde_worker_regen/process_management/process_manager.py https://raw.githubusercontent.com/Haidra-Org/horde-worker-reGen/main/horde_worker_regen/process_management/process_manager.py\n",
        "#experimental = False\n",
        "\n",
        "process_manager_py_path = '/content/horde-worker-reGen/horde_worker_regen/process_management/process_manager.py'\n",
        "\n",
        "\n",
        "# only execute this if experimental is True\n",
        "if (experimental):\n",
        "    # read the file\n",
        "    with open(process_manager_py_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    found_line_number = -1\n",
        "\n",
        "    ##\n",
        "    ## popped job - search_string_1, new_code_1\n",
        "    search_string_1 = '        logger.info(f\"Popped job {job_pop_response.id_} (model: {job_pop_response.model})\")'\n",
        "    new_code_1 = r'''        logger.success(f\"Popped job {job_pop_response.id_} (model: {job_pop_response.model})\")\n",
        "'''\n",
        "\n",
        "    # find the target line \"search_string_1\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_1 in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_1\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_1)\n",
        "    ## popped job\n",
        "    ##\n",
        "    found_line_number = -1\n",
        "\n",
        "    ##\n",
        "    ## number and ID of loras - search_string_lora, new_code_lora\n",
        "    search_string_lora = '            if next_job.payload.loras:'\n",
        "    new_code_lora = r'''            ##\n",
        "            import re\n",
        "            matches = \"\"\n",
        "            matches = re.findall(r\"<lora:(-?\\d+:-?\\d+\\.\\d+:-?\\d+\\.\\d+).*?>\", next_job.payload.prompt)\n",
        "            lora_array = []\n",
        "            # Array containing all loras in the prompt\n",
        "            for match in matches:\n",
        "                lora_id = match.split(':')[0]\n",
        "                lora_array.append(lora_id)\n",
        "\n",
        "            if next_job.payload.loras or len(lora_array):\n",
        "                if extra_info:\n",
        "                    extra_info += \", \"\n",
        "                lora_payload_names = [entry.name for entry in next_job.payload.loras]\n",
        "                extra_info += f\"{len(next_job.payload.loras)} (payload: {lora_payload_names if lora_payload_names else 'None'}) and {len(lora_array)} (injection: {lora_array if lora_array else 'None'}) LoRAs\"\n",
        "            ##\n",
        "'''\n",
        "\n",
        "    # find the target line \"search_string_lora\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_lora in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_lora\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_lora)\n",
        "    ## number and ID of loras\n",
        "    ##\n",
        "    found_line_number = -1\n",
        "\n",
        "    ##\n",
        "    ## number and ID of tis - search_string_ti, new_code_ti\n",
        "    search_string_ti = '            if next_job.payload.tis:'\n",
        "    new_code_ti = r'''            ##\n",
        "            matches = \"\"\n",
        "            matches = re.findall(r\"embedding:(-?\\d+:-?\\d+\\.\\d+)\", next_job.payload.prompt)\n",
        "            ti_array = []\n",
        "            # Array containing all embeddings in the prompt\n",
        "            for match in matches:\n",
        "                ti_id = match.split(':')[0]\n",
        "                ti_array.append(ti_id)\n",
        "\n",
        "            if next_job.payload.tis or len(ti_array):\n",
        "                if extra_info:\n",
        "                    extra_info += \", \"\n",
        "                ti_payload_names = [entry.name for entry in next_job.payload.tis]\n",
        "                extra_info += f\"{len(next_job.payload.tis)} (payload: {ti_payload_names if ti_payload_names else 'None'}) and {len(ti_array)} (injection: {ti_array if ti_array else 'None'}) TIs\"\n",
        "            ##\n",
        "'''\n",
        "\n",
        "    # find the target line \"search_string_ti\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_ti in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_ti\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_ti)\n",
        "    ## number and ID of tis\n",
        "    ##\n",
        "    found_line_number = -1\n",
        "\n",
        "    ##\n",
        "    ## extra info - search_string_extra, new_code_extra\n",
        "    search_string_extra = '                logger.info(extra_info)'\n",
        "    new_code_extra = f'''                logger.success(extra_info)\\n'''\n",
        "\n",
        "    ### to allow login-gated lora\n",
        "    # find the target line \"search_string_extra\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string_extra in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code_extra\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.pop(found_line_number)\n",
        "        lines.insert(found_line_number, new_code_extra)\n",
        "    ## extra info\n",
        "    ##\n",
        "\n",
        "    # rewrite process_manager.py\n",
        "    with open(process_manager_py_path, 'w') as file:\n",
        "        file.writelines(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwMyg3JN1xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 6.- Run download models { display-mode: \"form\" }\n",
        "\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "!source ../regen/bin/activate;python download_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRnzqILb1xPw",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# @title 7.- Run the worker { display-mode: \"form\" }\n",
        "\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "if (logs):\n",
        "    !source ../regen/bin/activate;python run_worker.py -vv\n",
        "else:\n",
        "    !source ../regen/bin/activate;python run_worker.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
