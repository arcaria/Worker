{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE1BHRAe1xPq"
      },
      "source": [
        "# Install the New Worker- BETA!!\n",
        "### Remember to first set your API KEY and worker name\n",
        "#### You can serve different models, simply change the name in models_to_load to match the model you want, you can check either https://aqualxx.github.io/stable-ui/workers, in the models tab, or https://tinybots.net/artbot/info/models, just copy paste the name (it's set to Deliberate by default)\n",
        "#### You can also change max_power and see how high you can go, it's set to 20 by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-20T23:06:35.113455Z",
          "iopub.status.busy": "2023-10-20T23:06:35.112701Z"
        },
        "id": "qHYIcsqJ1xPs",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#0.- This cell will set the variables and rerun the worker if you stopped it, but only if everything else was installed already\n",
        "\n",
        "#### For right now, THESE are the only variables that we care about\n",
        "worker_name = \"\"\n",
        "api_key = \"\"\n",
        "max_power = 20\n",
        "safety_on_gpu = True\n",
        "models_to_load = [\"Deliberate\"]\n",
        "allow_lora = True\n",
        "allow_controlnet = True\n",
        "nsfw = True\n",
        "censor_nsfw = False\n",
        "\n",
        "##############################\n",
        "##### HIGHLY EXPERIMENTAL - OPTIONAL, disabled by default ####\n",
        "# Colab exclusive, for now, inject loras and textual inversions through prompt so they can be used in similar fashion to A1111\n",
        "# for loras: (lora:civitaiID:lora_strength:lora_clip)\n",
        "# for textual inversions: (embedding:civitaiID:ti_strength)\n",
        "experimental = False\n",
        "# why? because some front-ends don't support loras/tis, so instead we let the worker handle that part and now EVERYONE can use them,\n",
        "# no  matter what client they use to access the horde\n",
        "#### HIGHLY EXPERIMENTAL - OPTIONAL, disabled by default  ####\n",
        "##############################\n",
        "\n",
        "####For right now, THESE are the only variables that we care about\n",
        "\n",
        "\n",
        "queue_size = 1\n",
        "max_threads = 1\n",
        "horde_url = \"https://aihorde.net\"\n",
        "allow_painting = False\n",
        "dynamic_models = False\n",
        "models_to_skip = [\"stable_diffusion_inpainting\", \"stable_diffusion_2.1\",  \"stable_diffusion_2.0\"]\n",
        "allow_post_processing = False\n",
        "priority_usernames = []\n",
        "blacklist = []\n",
        "censorlist = []\n",
        "allow_img2img = True\n",
        "allow_unsafe_ip = True\n",
        "number_of_dynamic_models = 0\n",
        "max_models_to_download = 10\n",
        "forms = [\"caption\",\"nsfw\",\"interrogation\",\"post-process\"]\n",
        "\n",
        "\n",
        "current_path = \"/content/\"\n",
        "worker_path = current_path + \"horde-worker-reGen/\"\n",
        "bridgeData_file = worker_path + \"bridgeData.yaml\"\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(bridgeData_file):\n",
        "    print (\"bridgeData.yaml file not found. Proceed to install the worker.\")\n",
        "else:\n",
        "    print (\"bridgeData.yaml file found. Recreating bridgeData.yaml and restarting the worker.\")\n",
        "    create_yaml()\n",
        "    !cd /content\n",
        "    !source ../regen/bin/activate;python download_models.py\n",
        "    !cd /content\n",
        "    !source ../regen/bin/activate;python run_worker.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVecpjOM1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#1.- Virtual Environment\n",
        "\n",
        "!apt-get update\n",
        "!apt install python3.10-venv\n",
        "!python -m venv regen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzwQDVd_1xPu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#2.- Remove the worker if it exists, then, Clone the regen worker\n",
        "\n",
        "!cd /content;rm -r horde-worker-reGen\n",
        "!cd /content;git clone https://github.com/Haidra-Org/horde-worker-reGen.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC4lce-51xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#3.- Install requirements for cuda 11.8\n",
        "\n",
        "!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.118.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUSwcaXc1xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#4.- Work around so hordelib installs @ cuda 11.8\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "!source regen/bin/activate;pip install -r .\\/horde-worker-reGen/requirements.hordelib.txt\n",
        "\n",
        "!source regen/bin/activate;pip install hordelib --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-I1XNxx1xPv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#5.- Create .yaml config file\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "%cd $worker_path\n",
        "print (\"Creating bridgeData.yaml file.\")\n",
        "\n",
        "def create_yaml():\n",
        "\n",
        "    from yaml import load, dump\n",
        "\n",
        "    def make_yaml_sublist(list_to_convert: list[str]):\n",
        "        sublist_yaml = dump(list_to_convert)\n",
        "        sublist_yaml = \"\\n\" + sublist_yaml\n",
        "        return sublist_yaml\n",
        "\n",
        "\n",
        "\n",
        "    data = f\"\"\"horde_url: \"{horde_url}\"\n",
        "api_key: \"{api_key}\"\n",
        "priority_usernames: []\n",
        "max_threads: {max_threads}\n",
        "queue_size: {queue_size}\n",
        "safety_on_gpu: {safety_on_gpu}\n",
        "require_upfront_kudos: false\n",
        "dreamer_name: \"{worker_name}\"\n",
        "max_power: {max_power}\n",
        "nsfw: {nsfw.__str__().lower()}\n",
        "censor_nsfw: {censor_nsfw}\n",
        "blacklist: {blacklist}\n",
        "censorlist: {censorlist}\n",
        "allow_img2img: {allow_img2img.__str__().lower()}\n",
        "allow_painting: {allow_painting.__str__().lower()}\n",
        "allow_unsafe_ip: true\n",
        "allow_post_processing: {allow_post_processing.__str__().lower()}\n",
        "allow_controlnet: {allow_controlnet.__str__().lower()}\n",
        "allow_lora: {allow_lora.__str__().lower()}\n",
        "max_lora_cache_size: 10\n",
        "dynamic_models: false\n",
        "number_of_dynamic_models: 0\n",
        "max_models_to_download: 10\n",
        "stats_output_frequency: 30\n",
        "cache_home: \"./\"\n",
        "always_download: true\n",
        "temp_dir: \"./tmp\"\n",
        "disable_terminal_ui: True\n",
        "vram_to_leave_free: \"80%\"\n",
        "ram_to_leave_free: \"80%\"\n",
        "disable_disk_cache: false\n",
        "models_to_load: {make_yaml_sublist(models_to_load)}\n",
        "models_to_skip: {make_yaml_sublist(models_to_skip)}\n",
        "suppress_speed_warnings: false\n",
        "forms:\n",
        "- \"caption\"\n",
        "- \"nsfw\"\n",
        "- \"interrogation\"\n",
        "- \"post-process\"\n",
        "\"\"\"\n",
        "\n",
        "    with open(bridgeData_file, \"w\") as text_file:\n",
        "        text_file.write(data)\n",
        "\n",
        "    print (\"bridgeData.yaml file created.\")\n",
        "\n",
        "create_yaml()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### EXPERIMENTAL ###\n",
        "\n",
        "# To undo the experimental changes and disable the loras/tis injection, uncomment the next 2 lines (delete the #) and run this cell again, then rerun the worker (the last cell)\n",
        "#!wget -O /content/regen/lib/python3.10/site-packages/hordelib/horde.py https://raw.githubusercontent.com/Haidra-Org/hordelib/main/hordelib/horde.py\n",
        "#experimental = False\n",
        "\n",
        "\n",
        "# To inject loras and tis through the prompt, we search in horde.py for \"search_string\" and write \"new_code\" above it, just because\n",
        "\n",
        "horde_py_path = '/content/regen/lib/python3.10/site-packages/hordelib/horde.py'\n",
        "search_string = '        # Negative and positive prompts are merged together'\n",
        "new_code = '''\n",
        "        #\n",
        "        # To inject Loras and TIs through the prompt\n",
        "        # Import regular expressions\n",
        "        import re\n",
        "\n",
        "        # Get the lora(s) from the prompt\n",
        "        matches = re.findall(r\"<lora:(-?\\d+:-?\\d+\\.\\d+:-?\\d+\\.\\d+).*>\", payload.get(\"prompt\"))\n",
        "        lora_array = []\n",
        "        # Array containing all loras in the prompt\n",
        "        for match in matches:\n",
        "            lora_array.append(match)\n",
        "        # Remove the lora(s) from the prompt\n",
        "        payload[\"prompt\"] = re.sub(r\"<lora:-?\\d+:-?\\d+\\.\\d+:-?\\d+\\.\\d+.*>\", \"\", payload[\"prompt\"])\n",
        "        \n",
        "        # Append the new lora(s) to payload[\"loras\"] \n",
        "        for lora_string in lora_array:\n",
        "            attributes = [\"name\", \"model\", \"clip\"]\n",
        "            values = lora_string.split(':')\n",
        "            lora_dict = dict(zip(attributes, values))\n",
        "            payload[\"loras\"].append(lora_dict)\n",
        "\n",
        "\n",
        "        # Get the ti(s) from the prompt\n",
        "        matches = re.findall(r\"embedding:(-?\\d+:-?\\d+\\.\\d+)\", payload.get(\"prompt\"))\n",
        "        # if inject_ti is necessary, use this next line instead\n",
        "        #matches = re.findall(r\"embedding:(-?\\d+:-?\\d+\\.\\d+:[\\w.]+)\", payload.get(\"prompt\"))\n",
        "        ti_array = []\n",
        "        # Array containing all loras in the prompt\n",
        "        for match in matches:\n",
        "            ti_array.append(match)\n",
        "        \n",
        "        # Append the new TI(s) to payload[\"tis\"] \n",
        "        for ti_string in ti_array:\n",
        "            attributes = [\"name\", \"strength\"]\n",
        "            # if inject_ti is necessary, use this next line instead\n",
        "            #attributes = [\"name\", \"inject_ti\", \"strength\"]\n",
        "            values = ti_string.split(':')\n",
        "            ti_dict = dict(zip(attributes, values))\n",
        "            payload[\"tis\"].append(ti_dict)\n",
        "\n",
        "        # To inject Loras and TIs through the prompt\n",
        "        #\n",
        "'''\n",
        "\n",
        "# only execute this if experimental is True\n",
        "if (experimental):\n",
        "    # read the file\n",
        "    with open(horde_py_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    found_line_number = -1\n",
        "\n",
        "    # find the target line \"search_string\"\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_string in line:\n",
        "            found_line_number = i\n",
        "            break\n",
        "\n",
        "    # it should always find it, but whatever, insert \"new_code\" to the data\n",
        "    if found_line_number != -1:\n",
        "        lines.insert(found_line_number - 1, new_code)\n",
        "\n",
        "    # rewrite horde.py\n",
        "    with open(horde_py_path, 'w') as file:\n",
        "        file.writelines(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwMyg3JN1xPv",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#6.- Run download models\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "!source ../regen/bin/activate;python download_models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRnzqILb1xPw",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#7.- Run the worker\n",
        "# Make sure you have the correct path based on any `cd` commands above\n",
        "!cd /content\n",
        "\n",
        "!source ../regen/bin/activate;python run_worker.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
